# Proxy API Configuration
PROXY_API_KEY=change-me-hehehoho

# vLLM Upstream Configuration
OPENAI_BASE_URL=http://vllm:8000/v1

# Model Configuration
DEFAULT_MODEL=Qwen/Qwen2.5-1.5B-Instruct
TP_SIZE=1
MAX_MODEL_LEN=4096

# HuggingFace Token (required for gated models like Llama)
HF_TOKEN=hf_xxxxxxxxxxxxxxxxx

# CORS Configuration (comma-separated origins)
CORS_ORIGINS=http://localhost:3000,http://localhost:8787

# Rate Limiting
RATE_LIMIT_PER_MINUTE=600
