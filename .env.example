# Proxy API Configuration
PROXY_API_KEY=change-me

# vLLM Upstream Configuration
OPENAI_BASE_URL=http://vllm:8000/v1

# Model Configuration
# Leave DEFAULT_MODEL blank for first-time setup
# You'll be prompted to select a model from downloaded models
DEFAULT_MODEL=
TP_SIZE=1
MAX_MODEL_LEN=4096

# HuggingFace Token (required for gated models like Llama)
HF_TOKEN=hf_xxxxxxxxxxxxxxxxx

# CORS Configuration (comma-separated origins)
CORS_ORIGINS=http://localhost:3000,http://localhost:8787

# Rate Limiting
RATE_LIMIT_PER_MINUTE=600
