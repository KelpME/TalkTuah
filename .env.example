# Proxy API Configuration
PROXY_API_KEY=change-me

# vLLM Upstream Configuration
OPENAI_BASE_URL=http://vllm:8000/v1

# Model Configuration
# Start with a small model for testing
DEFAULT_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
TP_SIZE=1
MAX_MODEL_LEN=4096

# HuggingFace Token (required for gated models like Llama)
# Get yours at: https://huggingface.co/settings/tokens
HF_TOKEN=hf_get_your_own

# ROCm Configuration (added for AMD GPU support)
HSA_OVERRIDE_GFX_VERSION=11.0.0

# CORS Configuration (comma-separated origins)
CORS_ORIGINS=http://localhost:3000,http://localhost:8787

# Rate Limiting
RATE_LIMIT_PER_MINUTE=600
