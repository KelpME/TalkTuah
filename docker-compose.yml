services:
  vllm:
    image: rocm/vllm:latest
    container_name: vllm-server
    command:
      - vllm
      - serve
      - ${DEFAULT_MODEL}
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
      - --dtype
      - float16
      - --max-model-len
      - "${MAX_MODEL_LEN:-4096}"
      - --tensor-parallel-size
      - "${TP_SIZE:-1}"
      - --gpu-memory-utilization
      - "0.75"
      - --max-num-seqs
      - "16"
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/workspace/models
      - HSA_OVERRIDE_GFX_VERSION=11.0.0
      - ROCM_PATH=/opt/rocm
      - HIP_VISIBLE_DEVICES=0
    volumes:
      - ./models:/workspace/models
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "bash", "-c", "curl -sf http://localhost:8000/v1/models >/dev/null"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 60s
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      - video
      - render
    security_opt:
      - seccomp:unconfined
    restart: "no"

  api:
    build:
      context: ./apps/api
      dockerfile: Dockerfile
    container_name: vllm-proxy-api
    environment:
      - PROXY_API_KEY=${PROXY_API_KEY}
      - UPSTREAM_BASE_URL=${OPENAI_BASE_URL}
      - CORS_ORIGINS=${CORS_ORIGINS:-*}
      - RATE_LIMIT_PER_MINUTE=${RATE_LIMIT_PER_MINUTE:-60}
      - MODELS_DIR=/workspace/models
    ports:
      - "8787:8787"
    volumes:
      - .:/workspace
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      vllm:
        condition: service_started
    restart: "no"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8787/api/healthz"]
      interval: 60s
      timeout: 10s
      retries: 3
